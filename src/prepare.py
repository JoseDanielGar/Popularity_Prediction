# %% [markdown]
# # Preparaci√≥n de Datos para Modelado
# 
# Este notebook realiza la preparaci√≥n final de los datos para el entrenamiento de modelos de machine learning:
# 
# - Cargar librer√≠as necesarias
# - Identificar y codificar variables categ√≥ricas
# - Dividir el conjunto de datos en entrenamiento y prueba
# - Guardar los conjuntos de datos procesados
# 
# **Nota:** Se asume que los datos ya han sido limpiados previamente.

# %% [markdown]
# ## 1. Cargar Librer√≠as Requeridas
# 
# Importar todas las librer√≠as necesarias para la preparaci√≥n de datos y codificaci√≥n de variables.

# %%
import pandas as pd
import numpy as np
import yaml
import os
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler
import joblib
from datetime import datetime

# Suprimir warnings para una salida m√°s limpia
warnings.filterwarnings('ignore')

# Configuraci√≥n de pandas para mejor visualizaci√≥n
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', 50)

print("‚úì Librer√≠as importadas exitosamente!")
print(f"Pandas versi√≥n: {pd.__version__}")
print(f"NumPy versi√≥n: {np.__version__}")

# %% [markdown]
# ## 2. Cargar Configuraci√≥n y Datos
# 
# Cargar los par√°metros desde el archivo `params.yaml` y el conjunto de datos limpio.

# %%
# Cargar configuraci√≥n desde params.yaml
config = yaml.safe_load(open("params.yaml"))["prepare"]

print("Configuraci√≥n cargada:")
print("=" * 40)
for key, value in config.items():
    print(f"{key}: {value}")

# Extraer par√°metros
input_path = config["input_path"]
output_path_train = config["output_path_train"]
output_path_test = config["output_path_test"]
test_split = config["split"]
random_seed = config["seed"]

print(f"\n‚úì Configuraci√≥n cargada exitosamente!")
print(f"‚úì Divisi√≥n de test: {test_split*100}%")
print(f"‚úì Semilla aleatoria: {random_seed}")

# %%
# Cargar el conjunto de datos limpio
print(f"Cargando datos desde: {input_path}")

if os.path.exists(input_path):
    df = pd.read_csv(input_path)
    print(f"‚úì Datos cargados exitosamente!")
    print(f"‚úì Forma del conjunto de datos: {df.shape}")
    print(f"‚úì Total de registros: {len(df):,}")
    print(f"‚úì Total de caracter√≠sticas: {df.shape[1]}")
else:
    raise FileNotFoundError(f"Archivo no encontrado: {input_path}")

# Establecer semilla para reproducibilidad
np.random.seed(random_seed)
print(f"\n‚úì Semilla aleatoria establecida: {random_seed}")

# %% [markdown]
# ## 3. Exploraci√≥n de Variables
# 
# Analizar el conjunto de datos para identificar tipos de variables y su distribuci√≥n.

# %%
# Mostrar informaci√≥n general del conjunto de datos
print("INFORMACI√ìN GENERAL DEL CONJUNTO DE DATOS")
print("=" * 50)
print(f"Forma: {df.shape}")
print(f"\nPrimeras 5 filas:")
print(df.head())

print(f"\nInformaci√≥n de tipos de datos:")
print(df.info())

# %%
df.describe(include='all')

# %%
# Identificar tipos de variables
print("AN√ÅLISIS DE TIPOS DE VARIABLES")
print("=" * 50)

# Variables categ√≥ricas (object y category)
categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()
categorical_columns += ['key', 'mode', 'time_signature']
print(f"\nVariables categ√≥ricas ({len(categorical_columns)}):")
for col in categorical_columns:
    unique_values = df[col].nunique()
    print(f"  - {col}: {df[col].dtype} (valores √∫nicos: {unique_values})")

# Variables num√©ricas
numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
# Excluir columnas num√©ricas que est√°n en categorical_columns
numeric_columns = [col for col in numeric_columns if col not in categorical_columns]
print(f"Variables num√©ricas ({len(numeric_columns)}):")
for col in numeric_columns:
    print(f"  - {col}: {df[col].dtype}")

# Variables booleanas
boolean_columns = df.select_dtypes(include=['bool']).columns.tolist()
print(f"\nVariables booleanas ({len(boolean_columns)}):")
for col in boolean_columns:
    print(f"  - {col}: {df[col].dtype}")

# %% [markdown]
# ## 4. Normalizaci√≥n de Variables Num√©ricas
# 
# Analizar y normalizar las variables num√©ricas para mejorar el rendimiento de los algoritmos de machine learning que son sensibles a la escala de las caracter√≠sticas.

# %%
# An√°lisis estad√≠stico de variables num√©ricas
columns_to_normalize = ['duration_ms','loudness', 'tempo']

print("AN√ÅLISIS ESTAD√çSTICO DE VARIABLES NUM√âRICAS A NORMALIZAR")
print("=" * 50)

# Estad√≠sticas descriptivas de variables num√©ricas
numeric_stats = df[columns_to_normalize].describe()
print(numeric_stats)

print(f"\nRangos de valores para cada variable num√©rica a normalizar:")
print("-" * 50)
for col in columns_to_normalize:
    min_val = df[col].min()
    max_val = df[col].max()
    mean_val = df[col].mean()
    std_val = df[col].std()
    print(f"{col}:")
    print(f"  ‚Ä¢ Rango: [{min_val:.3f}, {max_val:.3f}]")
    print(f"  ‚Ä¢ Media: {mean_val:.3f}, Desv. Est√°ndar: {std_val:.3f}")
    ratio = max_val / min_val if min_val != 0 else 'Inf'
    if isinstance(ratio, str):
        print(f"  ‚Ä¢ Ratio (max/min): {ratio}")
    else:
        print(f"  ‚Ä¢ Ratio (max/min): {ratio:.2f}")
    print()

# %%
# Determinaci√≥n de estrategia de normalizaci√≥n
if columns_to_normalize:
    print("DETERMINACI√ìN DE ESTRATEGIA DE NORMALIZACI√ìN")
    print("=" * 50)
    
    # An√°lisis de distribuciones para determinar el mejor m√©todo
    variables_to_normalize = []
    normalization_strategy = {}
    
    for col in columns_to_normalize:
        # Calcular estad√≠sticas para determinar el m√©todo apropiado
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        
        # Detectar outliers usando IQR
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]
        outlier_percentage = len(outliers) / len(df) * 100
        
        # Determinar el rango de valores
        min_val = df[col].min()
        max_val = df[col].max()
        range_ratio = max_val / min_val if min_val > 0 else float('inf')
        
        # Decidir m√©todo de normalizaci√≥n
        if outlier_percentage > 5:  # Si hay muchos outliers, usar RobustScaler
            method = "RobustScaler"
            scaler = RobustScaler()
        elif range_ratio > 100:  # Si el rango es muy amplio, usar StandardScaler
            method = "StandardScaler"
            scaler = StandardScaler()
        else:  # Para rangos moderados, usar MinMaxScaler
            method = "MinMaxScaler"
            scaler = MinMaxScaler()
        
        variables_to_normalize.append(col)
        normalization_strategy[col] = {'method': method, 'scaler': scaler, 'outlier_pct': outlier_percentage}
        
        print(f"‚úì {col}: {method}")
        print(f"  ‚Ä¢ Outliers: {outlier_percentage:.1f}%")
        print(f"  ‚Ä¢ Ratio rango: {range_ratio:.2f}")
        print()
    
    print(f"Total de variables a normalizar: {len(variables_to_normalize)}")
else:
    variables_to_normalize = []
    normalization_strategy = {}
    print("‚úì No hay variables num√©ricas para normalizar.")

# %%
# Aplicar normalizaci√≥n a las variables num√©ricas
df_normalized = df.copy()
scalers = {}  # Diccionario para guardar los scalers

if variables_to_normalize:
    print("APLICANDO NORMALIZACI√ìN")
    print("=" * 50)
    
    for col in variables_to_normalize:
        method = normalization_strategy[col]['method']
        scaler = normalization_strategy[col]['scaler']
        
        # Aplicar normalizaci√≥n
        original_values = df[col].values.reshape(-1, 1)
        normalized_values = scaler.fit_transform(original_values)
        df_normalized[col] = normalized_values.flatten()
        
        # Guardar el scaler para uso futuro
        scalers[f'{col}_scaler'] = scaler
        
        # Mostrar estad√≠sticas antes y despu√©s
        original_mean = df[col].mean()
        original_std = df[col].std()
        normalized_mean = df_normalized[col].mean()
        normalized_std = df_normalized[col].std()
        
        print(f"‚úì {col} ({method}):")
        print(f"  ‚Ä¢ Antes: Œº={original_mean:.3f}, œÉ={original_std:.3f}")
        print(f"  ‚Ä¢ Despu√©s: Œº={normalized_mean:.3f}, œÉ={normalized_std:.3f}")
        print(f"  ‚Ä¢ Rango despu√©s: [{df_normalized[col].min():.3f}, {df_normalized[col].max():.3f}]")
        print()
    
    print(f"‚úì Normalizaci√≥n completada!")
    print(f"‚úì Variables normalizadas: {len(variables_to_normalize)}")
    print(f"‚úì Scalers guardados: {len(scalers)}")
else:
    print("‚úì No hay variables para normalizar.")

# %% [markdown]
# ## 5. Identificar y Codificar Variables Categ√≥ricas
# 
# Procesar las variables categ√≥ricas utilizando t√©cnicas de codificaci√≥n apropiadas.

# %%
# An√°lisis detallado de variables categ√≥ricas
if categorical_columns:
    print("AN√ÅLISIS DETALLADO DE VARIABLES CATEG√ìRICAS")
    print("=" * 50)
    
    for col in categorical_columns:
        unique_count = df[col].nunique()
        sample_values = df[col].value_counts().head(10)
        
        print(f"\nVariable: {col}")
        print(f"Valores √∫nicos: {unique_count}")
        print(f"Top 10 valores m√°s frecuentes:")
        print(sample_values)
        print("-" * 30)
else:
    print("‚úì No se encontraron variables categ√≥ricas para procesar.")

# %%
# Determinar estrategia de codificaci√≥n para cada variable categ√≥rica
if categorical_columns:
    print("ESTRATEGIA DE CODIFICACI√ìN")
    print("=" * 50)
    
    # Separar variables por cardinalidad
    low_cardinality = []  # Para One-Hot Encoding (<=10 categor√≠as)
    high_cardinality = [] # Para Label Encoding (>10 categor√≠as)
    
    for col in categorical_columns:
        unique_count = df[col].nunique()
        if unique_count <= 10:
            low_cardinality.append(col)
            print(f"‚úì {col}: One-Hot Encoding (cardinalidad: {unique_count})")
        else:
            high_cardinality.append(col)
            print(f"‚úì {col}: Label Encoding (cardinalidad: {unique_count})")
    
    print(f"\nResumen:")
    print(f"Variables para One-Hot Encoding: {len(low_cardinality)}")
    print(f"Variables para Label Encoding: {len(high_cardinality)}")
else:
    low_cardinality = []
    high_cardinality = []
    print("‚úì No hay variables categ√≥ricas para codificar.")

# %%
# Aplicar codificaci√≥n a variables categ√≥ricas
df_encoded = df_normalized.copy()  # Usar el dataframe normalizado
encoders = {}  # Diccionario para guardar los encoders

print("APLICANDO CODIFICACI√ìN")
print("=" * 50)

# Label Encoding para variables de alta cardinalidad
if high_cardinality:
    print("Aplicando Label Encoding...")
    for col in high_cardinality:
        le = LabelEncoder()
        df_encoded[col] = le.fit_transform(df_normalized[col].astype(str))
        encoders[f'{col}_label_encoder'] = le
        print(f"  ‚úì {col}: {df_normalized[col].nunique()} categor√≠as ‚Üí valores num√©ricos")

# One-Hot Encoding para variables de baja cardinalidad
if low_cardinality:
    print("\nAplicando One-Hot Encoding...")
    for col in low_cardinality:
        # Crear variables dummy
        dummies = pd.get_dummies(df_normalized[col], prefix=col, drop_first=True)
        
        # Remover columna original y agregar las nuevas
        df_encoded = df_encoded.drop(columns=[col])
        df_encoded = pd.concat([df_encoded, dummies], axis=1)
        
        # Guardar las columnas creadas para referencia
        encoders[f'{col}_dummy_columns'] = dummies.columns.tolist()
        print(f"  ‚úì {col}: {df_normalized[col].nunique()} categor√≠as ‚Üí {len(dummies.columns)} variables dummy")

print(f"\n‚úì Codificaci√≥n completada!")
print(f"‚úì Forma despu√©s de normalizaci√≥n: {df_normalized.shape}")
print(f"‚úì Forma despu√©s de codificaci√≥n: {df_encoded.shape}")
print(f"‚úì Nuevas caracter√≠sticas creadas: {df_encoded.shape[1] - df_normalized.shape[1]}")

# %% [markdown]
# ## 6. Divisi√≥n en Conjuntos de Entrenamiento y Prueba
# 
# Dividir el conjunto de datos codificado en conjuntos de entrenamiento y prueba seg√∫n los par√°metros configurados.

# %%
# Verificar si existe una columna objetivo (target)
print("IDENTIFICACI√ìN DE VARIABLE OBJETIVO")
print("=" * 50)

target_column = 'popularity'
print(f"‚úì Variable objetivo seleccionada: {target_column}")

# Separar caracter√≠sticas (X) y variable objetivo (y)
X = df_encoded.drop(columns=[target_column])
y = df_encoded[target_column]

print(f"\nForma de caracter√≠sticas (X): {X.shape}")
print(f"Forma de variable objetivo (y): {y.shape}")
print(f"\nDistribuci√≥n de la variable objetivo:")
print(y.value_counts().sort_index())

# %%
# Realizar la divisi√≥n train/test
print("DIVISI√ìN EN CONJUNTOS DE ENTRENAMIENTO Y PRUEBA")
print("=" * 50)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=test_split, 
    random_state=random_seed,
    #stratify=y if len(y.unique()) > 1 and len(y.unique()) < len(y) else None
)

print(f"‚úì Divisi√≥n completada con semilla: {random_seed}")
print(f"‚úì Tama√±o de divisi√≥n de prueba: {test_split*100}%")
print(f"\nTama√±os de conjuntos:")
print(f"  - Entrenamiento: {len(X_train):,} registros ({len(X_train)/len(X)*100:.1f}%)")
print(f"  - Prueba: {len(X_test):,} registros ({len(X_test)/len(X)*100:.1f}%)")

print(f"\nDistribuci√≥n en conjunto de entrenamiento:")
print(y_train.value_counts().sort_index())

print(f"\nDistribuci√≥n en conjunto de prueba:")
print(y_test.value_counts().sort_index())

# %% [markdown]
# ## 7. Guardar Conjuntos de Datos Procesados
# 
# Exportar los conjuntos de entrenamiento y prueba a los directorios especificados en la configuraci√≥n.

# %%
# Crear directorios de salida
print("GUARDANDO CONJUNTOS DE DATOS PROCESADOS")
print("=" * 50)

os.makedirs(output_path_train, exist_ok=True)
os.makedirs(output_path_test, exist_ok=True)

print(f"‚úì Directorios creados:")
print(f"  - Entrenamiento: {output_path_train}")
print(f"  - Prueba: {output_path_test}")

# %%
# Guardar conjunto de entrenamiento
train_features_path = os.path.join(output_path_train, 'X_train.csv')
train_target_path = os.path.join(output_path_train, 'y_train.csv')

X_train.to_csv(train_features_path, index=False)
y_train.to_csv(train_target_path, index=False)

print(f"‚úì Conjunto de entrenamiento guardado:")
print(f"  - Caracter√≠sticas: {train_features_path}")
print(f"  - Variable objetivo: {train_target_path}")
print(f"  - Registros: {len(X_train):,}")
print(f"  - Caracter√≠sticas: {X_train.shape[1]}")

# %%
# Guardar conjunto de prueba
test_features_path = os.path.join(output_path_test, 'X_test.csv')
test_target_path = os.path.join(output_path_test, 'y_test.csv')

X_test.to_csv(test_features_path, index=False)
y_test.to_csv(test_target_path, index=False)

print(f"‚úì Conjunto de prueba guardado:")
print(f"  - Caracter√≠sticas: {test_features_path}")
print(f"  - Variable objetivo: {test_target_path}")
print(f"  - Registros: {len(X_test):,}")
print(f"  - Caracter√≠sticas: {X_test.shape[1]}")

# %%
# Guardar encoders, scalers y metadata
encoders_path = os.path.join(output_path_train, 'encoders.joblib')
scalers_path = os.path.join(output_path_train, 'scalers.joblib')
metadata_path = os.path.join(output_path_train, 'metadata.yaml')

# Guardar encoders
if encoders:
    joblib.dump(encoders, encoders_path)
    print(f"‚úì Encoders guardados: {encoders_path}")

# Guardar scalers
if scalers:
    joblib.dump(scalers, scalers_path)
    print(f"‚úì Scalers guardados: {scalers_path}")

# Crear metadata
metadata = {
    'original_shape': df.shape,
    'encoded_shape': df_encoded.shape,
    'target_column': target_column,
    'numeric_columns': numeric_columns,
    'categorical_columns': categorical_columns,
    'low_cardinality_encoded': low_cardinality,
    'high_cardinality_encoded': high_cardinality,
    'train_size': len(X_train),
    'test_size': len(X_test),
    'test_split_ratio': test_split,
    'random_seed': random_seed,
    'feature_count': X_train.shape[1],
    'encoding_date': datetime.now().strftime('%Y-%m-%d')
}

with open(metadata_path, 'w') as f:
    yaml.dump(metadata, f, default_flow_style=False)

print(f"‚úì Metadata guardada: {metadata_path}")

# %% [markdown]
# ## 8. Resumen Final
# 
# Resumen completo del proceso de preparaci√≥n de datos.

# %%
# Resumen final del proceso
print("üéâ PREPARACI√ìN DE DATOS COMPLETADA EXITOSAMENTE")
print("=" * 60)

print(f"üìä ESTAD√çSTICAS GENERALES:")
print(f"  ‚Ä¢ Conjunto de datos original: {df.shape}")
print(f"  ‚Ä¢ Conjunto de datos normalizado: {df_normalized.shape}")
print(f"  ‚Ä¢ Conjunto de datos codificado: {df_encoded.shape}")
print(f"  ‚Ä¢ Nuevas caracter√≠sticas creadas: {df_encoded.shape[1] - df.shape[1]}")

print(f"\nüîß PROCESAMIENTO REALIZADO:")
print(f"  ‚Ä¢ Variables num√©ricas: {len(numeric_columns)}")
print(f"  ‚Ä¢ Variables normalizadas: {len(variables_to_normalize)}")
print(f"  ‚Ä¢ Variables categ√≥ricas procesadas: {len(categorical_columns)}")
print(f"  ‚Ä¢ One-Hot Encoding aplicado a: {len(low_cardinality)} variables")
print(f"  ‚Ä¢ Label Encoding aplicado a: {len(high_cardinality)} variables")

if variables_to_normalize:
    print(f"\nüìè NORMALIZACI√ìN APLICADA:")
    for col in variables_to_normalize:
        method = normalization_strategy[col]['method']
        print(f"  ‚Ä¢ {col}: {method}")

print(f"\nüìÅ DIVISI√ìN DE DATOS:")
print(f"  ‚Ä¢ Conjunto de entrenamiento: {len(X_train):,} registros ({len(X_train)/len(X)*100:.1f}%)")
print(f"  ‚Ä¢ Conjunto de prueba: {len(X_test):,} registros ({len(X_test)/len(X)*100:.1f}%)")
print(f"  ‚Ä¢ Variable objetivo: {target_column}")
print(f"  ‚Ä¢ Semilla utilizada: {random_seed}")

print(f"\nüíæ ARCHIVOS GENERADOS:")
print(f"  ‚Ä¢ {train_features_path}")
print(f"  ‚Ä¢ {train_target_path}")
print(f"  ‚Ä¢ {test_features_path}")
print(f"  ‚Ä¢ {test_target_path}")
if encoders:
    print(f"  ‚Ä¢ {encoders_path}")
if scalers:
    print(f"  ‚Ä¢ {scalers_path}")
print(f"  ‚Ä¢ {metadata_path}")

print(f"\n‚úÖ Los datos est√°n listos para el entrenamiento de modelos de machine learning!")


